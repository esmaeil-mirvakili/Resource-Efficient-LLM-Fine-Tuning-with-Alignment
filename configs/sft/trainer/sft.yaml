resume_from: null
eraly_stopping:
  early_stopping_patience: 3
  early_stopping_threshold: 0.0
training_arguments:
  # batch
  per_device_train_batch_size: 6
  per_device_eval_batch_size: 6
  gradient_accumulation_steps: 8
  # gradient
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: true
  max_grad_norm: 1.0
  # precision
  bf16: true
  # learning rate
  learning_rate: 2e-4
  lr_scheduler_type: cosine
  warmup_ratio: 0.05
  # training
  weight_decay: 0.0
  num_train_epochs: 3
  max_steps: -1
  optim: paged_adamw_8bit
  # evaluation
  do_eval: true
  eval_strategy: steps
  eval_steps: 200
  # logging
  logging_steps: 10
  logging_strategy: steps
  report_to:
    - wandb
  # deepspeed
  deepspeed: null
  ddp_find_unused_parameters: false
  # saving
  output_dir: ./outputs/sft_checkpoint
  save_strategy: steps
  save_steps: 200
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: eval_loss
  greater_is_better: false
  save_safetensors: true
  # HF Hub
  push_to_hub: false
  hub_model_id: null
  hub_strategy: end
  hub_private_repo: false